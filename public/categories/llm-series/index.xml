<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM Series on Abhilash&#39;s Space</title>
    <link>https://abhihash01.github.io/categories/llm-series/</link>
    <description>Recent content in LLM Series on Abhilash&#39;s Space</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Made with ‚ù§ Abhilash</copyright>
    <lastBuildDate>Sun, 23 Feb 2025 23:54:26 -0500</lastBuildDate><atom:link href="https://abhihash01.github.io/categories/llm-series/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GPT1</title>
      <link>https://abhihash01.github.io/post/genai/primers/llm/gpt1/</link>
      <pubDate>Sun, 23 Feb 2025 23:54:26 -0500</pubDate>
      
      <guid>https://abhihash01.github.io/post/genai/primers/llm/gpt1/</guid>
      <description>
        
          
            &lt;h1 id=&#34;technical-primer-improving-language-understanding-by-generative-pre-training-gpt-1&#34;&gt;Technical Primer: Improving Language Understanding by Generative Pre-Training (GPT-1)&lt;/h1&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This work demonstrates the effectiveness of generative pre-training followed by discriminative fine-tuning for natural language understanding tasks. Our approach achieves state-of-the-art results on 9 of 12 benchmarks using a task-agnostic transformer architecture, with absolute improvements up to 8.9% on commonsense reasoning and 5.7% on question answering[1].&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Key innovations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First application of transformer architecture for unsupervised pre-training&lt;/li&gt;
&lt;li&gt;Task-specific input transformations enabling minimal architectural changes&lt;/li&gt;
&lt;li&gt;Demonstration of effective knowledge transfer across diverse NLP tasks&lt;/li&gt;
&lt;li&gt;Analysis of zero-shot learning capabilities in pre-trained models[1]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-framework&#34;&gt;2. Framework&lt;/h2&gt;
&lt;h3 id=&#34;21-unsupervised-pre-training&#34;&gt;2.1 Unsupervised Pre-Training&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: 12-layer transformer decoder with:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Bert</title>
      <link>https://abhihash01.github.io/post/genai/primers/llm/bert/</link>
      <pubDate>Tue, 18 Feb 2025 20:54:26 -0500</pubDate>
      
      <guid>https://abhihash01.github.io/post/genai/primers/llm/bert/</guid>
      <description>
        
          
            &lt;h1 id=&#34;bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding&#34;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/h1&gt;
&lt;h2 id=&#34;core-innovation&#34;&gt;Core Innovation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Deep bidirectional context&lt;/strong&gt; through joint conditioning on left/right context in all layers, overcoming limitations of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ELMo&#39;s shallow concatenation&lt;/li&gt;
&lt;li&gt;GPT&#39;s unidirectional approach&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-architecture-components&#34;&gt;Key Architecture Components&lt;/h2&gt;
&lt;h3 id=&#34;model-specifications&#34;&gt;Model Specifications&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;BERT-Base&lt;/th&gt;
          &lt;th&gt;BERT-Large&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Layers&lt;/td&gt;
          &lt;td&gt;12&lt;/td&gt;
          &lt;td&gt;24&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Hidden Dimension&lt;/td&gt;
          &lt;td&gt;768&lt;/td&gt;
          &lt;td&gt;1024&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Attention Heads&lt;/td&gt;
          &lt;td&gt;12&lt;/td&gt;
          &lt;td&gt;16&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Total Parameters&lt;/td&gt;
          &lt;td&gt;110M&lt;/td&gt;
          &lt;td&gt;340M&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;input-representation&#34;&gt;Input Representation&lt;/h3&gt;
&lt;p&gt;
  &lt;figure&gt;
  &lt;picture&gt;

    
      
        
        
        
        
        
        
    &lt;img
      loading=&#34;lazy&#34;
      decoding=&#34;async&#34;
      alt=&#34;Input Embeddings Diagram&#34;
      
        class=&#34;image_figure image_internal image_processed&#34;
        width=&#34;1258&#34;
        height=&#34;372&#34;
        src=&#34;https://abhihash01.github.io/post/genai/primers/llm/bert/Bert_embedding.png&#34;
      
      
    /&gt;

    &lt;/picture&gt;
&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Three embedding types summation&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Token Embeddings&lt;/strong&gt;: WordPiece (30k vocabulary)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segment Embeddings&lt;/strong&gt;: Sentence A/B differentiation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Position Embeddings&lt;/strong&gt;: Learned positional encoding&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Special tokens:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
