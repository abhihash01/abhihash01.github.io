[{"body":"","link":"https://abhihash01.github.io/","section":"","tags":null,"title":""},{"body":"","link":"https://abhihash01.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://abhihash01.github.io/tags/gpt1/","section":"tags","tags":null,"title":"GPT1"},{"body":"Technical Primer: Improving Language Understanding by Generative Pre-Training (GPT-1) Abstract This work demonstrates the effectiveness of generative pre-training followed by discriminative fine-tuning for natural language understanding tasks. Our approach achieves state-of-the-art results on 9 of 12 benchmarks using a task-agnostic transformer architecture, with absolute improvements up to 8.9% on commonsense reasoning and 5.7% on question answering[1].\n1. Introduction Key innovations:\nFirst application of transformer architecture for unsupervised pre-training Task-specific input transformations enabling minimal architectural changes Demonstration of effective knowledge transfer across diverse NLP tasks Analysis of zero-shot learning capabilities in pre-trained models[1] 2. Framework 2.1 Unsupervised Pre-Training Architecture: 12-layer transformer decoder with:\n768-dimensional hidden states 12 attention heads 3072-dimensional feed-forward layers 512-token context window[1] Training Objective: [Insert Equation 1 from paper: Language modeling objective] Key Specifications:\nDataset: BooksCorpus (7,000+ books) Batch size: 64 sequences Byte Pair Encoding (40k merges) Adam optimization (max lr 2.5e-4)[1] 2.2 Supervised Fine-Tuning Adaptation Method: [Insert Equation 3 from paper: Fine-tuning objective] Task-Specific Input Formats: [Insert Figure 1(right) from paper: Input transformations] Task Type Format Textual Entailment [s] Premise [delim] Hypothesis Similarity [s] Text1 [delim] Text2 [extract] QA/Reasoning [s] Document [delim] Question [delim] Answer[1] 3. Experimental Results 3.1 Natural Language Inference Dataset Accuracy Improvement MNLI-m 82.1% +1.5% SNLI 89.9% +0.6% SciTail 88.3% +5.0% 3.2 Question Answering Dataset Accuracy Improvement RACE (Overall) 59.0% +5.7% Story Cloze 86.5% +8.9% 3.3 GLUE Benchmark Achieved 72.8 overall score (+3.9 over previous SOTA), including:\nCoLA: 45.4 Mathews correlation (+10.4) SST-2: 91.3% accuracy 4. Analysis 4.1 Layer Transfer Impact [Insert Figure 2(left) from paper: Layer transfer analysis] Three embedding types summation Full layer transfer provides 9% accuracy improvement on MultiNLI vs embedding-only transfer[1]\n4.2 Zero-Shot Learning [Insert Figure 2(right) from paper: Zero-shot progression] Pre-trained model achieves 45% accuracy on SST-2 without fine-tuning through heuristic:\nAppend \u0026quot;very\u0026quot; to input Compare model's P(\u0026quot;positive\u0026quot;) vs P(\u0026quot;negative\u0026quot;) 5. Key Architectural Insights Transformer Superiority: 5.6 average score improvement over LSTM baseline Auxiliary LM Objective: Critical for larger datasets (1-2% improvements) Positional Encodings: Learned embeddings outperformed sinusoidal variants Long-Range Context: 512-token window essential for document-level tasks[1] 6. Implementation Checklist Byte Pair Encoding with 40k merges Layer-wise learning rate adaptation Task-specific input formatting Attention masking for language modeling Gradient clipping (max norm 1.0) Learning rate warmup (2000 steps) 7. Critical Ablations Variation Performance Drop Remove pre-training 14.8% Remove transformer layers 9.0% Remove positional encoding 3.1% 8. Conclusion This work establishes that:\nGenerative pre-training enables effective knowledge transfer Transformers outperform LSTMs for contextual representations Task-specific architectures can be minimized through smart input formatting Language modeling objective captures diverse linguistic phenomena[1] Further Reading Original Paper: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf ","link":"https://abhihash01.github.io/post/genai/primers/llm/gpt1/","section":"post","tags":["LLM","GPT1"],"title":"GPT1"},{"body":"","link":"https://abhihash01.github.io/tags/index/","section":"tags","tags":null,"title":"Index"},{"body":"","link":"https://abhihash01.github.io/tags/llm/","section":"tags","tags":null,"title":"LLM"},{"body":"","link":"https://abhihash01.github.io/series/llm/","section":"series","tags":null,"title":"LLM"},{"body":"","link":"https://abhihash01.github.io/categories/llm-series/","section":"categories","tags":null,"title":"LLM Series"},{"body":"","link":"https://abhihash01.github.io/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://abhihash01.github.io/series/pretraining/","section":"series","tags":null,"title":"Pretraining"},{"body":"","link":"https://abhihash01.github.io/categories/pretraining/","section":"categories","tags":null,"title":"Pretraining"},{"body":"","link":"https://abhihash01.github.io/categories/primer/","section":"categories","tags":null,"title":"Primer"},{"body":"","link":"https://abhihash01.github.io/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"https://abhihash01.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Core Innovation Deep bidirectional context through joint conditioning on left/right context in all layers, overcoming limitations of:\nELMo's shallow concatenation GPT's unidirectional approach Key Architecture Components Model Specifications Parameter BERT-Base BERT-Large Layers 12 24 Hidden Dimension 768 1024 Attention Heads 12 16 Total Parameters 110M 340M Input Representation Three embedding types summation\nToken Embeddings: WordPiece (30k vocabulary) Segment Embeddings: Sentence A/B differentiation Position Embeddings: Learned positional encoding Special tokens:\n[CLS]: Classification token [SEP]: Sentence separator Pre Training- Fine Tuning Modelling\nPre-training Tasks 1. Masked Language Modeling (MLM) 15% tokens masked per sequence Masking Strategy: 80% ‚Üí [MASK] 10% ‚Üí Random token 10% ‚Üí Original token Solves pretraining-finetuning mismatch 2. Next Sentence Prediction (NSP) Binary classification task: 50% actual consecutive sentences 50% random sentence pairs Trains relationship understanding between text spans Training Details Data Sources BooksCorpus (800M words) English Wikipedia (2,500M words) Technical Specifications Training Hardware: 16 TPU pods Batch Size: 256 sequences √ó 512 tokens Adam Optimization (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999) Fine-tuning Approach Task-Specific Adaptations:\nSingle Sentence Tasks\nUse [CLS] token output for classification\nSentence Pair Tasks\nSeparate with [SEP] + segment embeddings\nToken-Level Tasks\nUse final hidden states directly\nImpact \u0026amp; Results Benchmark BERT-Large Previous SOTA Improvement GLUE 80.4% 72.7% +7.7% SQuAD v1.1 (F1) 93.2 91.6 +1.6 SWAG 86.3% 59.9% +26.4% Legacy \u0026amp; Evolution BERT's Influence:\nRoBERTa (Optimized pretraining) ALBERT (Parameter efficiency) DistilBERT (Knowledge distillation) Ethical Considerations Training Data: BookCorpus gender/cultural biases Carbon Footprint: ~1,500 kg CO‚ÇÇ equivalent Environmental Impact: 79 kWh per training hour Further Reading Original Paper: arXiv:1810.04805 Official GitHub: google-research/bert HuggingFace Implementation: transformers.BertModel ","link":"https://abhihash01.github.io/post/genai/primers/llm/bert/","section":"post","tags":["LLM","BERT"],"title":"Bert"},{"body":"","link":"https://abhihash01.github.io/tags/bert/","section":"tags","tags":null,"title":"BERT"},{"body":"Hi, I am a Data Scientist and Machine Learning Engineer with 3+ years of corporate experience, currently pursuing a Master‚Äôs in Data Science, and specializing in Generative AI, advanced NLP, CV, and recommendation systems.\nOne day while on my Research Assistant job, cramming through a set of research papers trying to understand a specific concept and come up with new ideas, I realised, I have a trail of knowledge that I am gaining through these search. But there is not place I keep it in an organised form. I learn a concept, apply, move on and then days later, I forget and try to scramble to get to the same resources that helped me build my concepts.\nHence I decided to make a central note repository of all the new concepts I learn. Whats a better way of accountability than to publish it at a central place. Hence the birth of this website.\nGoing forward, trying to take time out of my hectic schedule, I aim to continuously update stuff, mostly on Data Science, in an incremental curriculum manner, so anyone wanting to learn a concept like i did, can benefit the best.\nReach out to me for collaborations : abhihash01@gmail.com\n","link":"https://abhihash01.github.io/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://abhihash01.github.io/archives/","section":"","tags":null,"title":""},{"body":"","link":"https://abhihash01.github.io/tags/abhilash/","section":"tags","tags":null,"title":"Abhilash"},{"body":"","link":"https://abhihash01.github.io/tags/featured/","section":"tags","tags":null,"title":"Featured"},{"body":"","link":"https://abhihash01.github.io/tags/profile/","section":"tags","tags":null,"title":"Profile"},{"body":"","link":"https://abhihash01.github.io/categories/profile/","section":"categories","tags":null,"title":"Profile"},{"body":"FNU ABHILASH Data Scientist and Machine Learning Engineer with 3+ years of corporate experience, currently pursuing a Master‚Äôs in Data Science, and specializing in Generative AI, advanced NLP, CV, and recommendation systems.\nüìß abhihash01@gmail.com | üíº (https://www.linkedin.com/in/abhilash29998/) | üêô (https://github.com/abhihash01) | üåê https://abhihash01.github.io\nSkills Languages: Python, C/C++, Java ML/DL Tools: PyTorch, Keras, Tensorflow, Numpy, Scikit, OpenCV, Pandas, Seaborn, CNN, RNN, LSTM, XGBoost, CatBoost Concepts: Data modeling, Statistical modeling, Regression, Natural Language Processing, Image Processing, Reinforcement Learning Generative AI: Langchain, LLamaindex, Ollama, Deepspeed, Torchtune, GAN, VAE, Diffusion, PEFT, QLoRA, RAG, Mixture of Experts (MoE), CoT/ Prompt Tuning, SFT, DPO, PPO, LLM Fine Tuning, RAG, CUDA, High Performance Computing, vLLM,OpenAI/Gemini API Ops: Docker, Kafka, Kubernates, Spring Boot, Django, Flask, FastAPI, Selenium, JIRA, Postman, Matlab, R, SAS, A/B Testing Data: Spark, PySpark, Hadoop, SQL, MySQL, MongoDB, Snowflake, Airflow, MLFlow, AWS, GCP, Sagemaker, Looker, PowerBI Experience SAP Machine Learning Engineer Bangalore, India | Jun 2021 - Aug 2023\nDeveloped a NLP based Biz Text Intelligence Model with NLTK and spaCy achieving 72% case coverage. Added Sentence Transformers for Intent Detection, increasing coverage to 94%, covering both rule based and natural conversation cases. Built a multi email based automatic email responder with contrastive learning using a pooled Transformers + CNN architecture and extractive summarizers achieving 0.67 ROUGE-L score on generation, 0.24 MAE and 0.87 NDCG for recommendation. Authored a product wide feature engineering library for 4 history use cases and implemented the Lead and Opportunity scoring model. The model provides scores along with what feature influences, enabling flexible strategy and model explainability. Lead a team for the migration of 8 ML use cases from SAP MLF to SAP AI Core directly impacting a revenue increase of $3.4M. Communicated with consumers and stakeholders to understand business requirements and spearheaded product roadmaps. Tech: Transformers, Catboost, PyTorch, Keras, Docker, Kubernates, AWS, Cloud Foundry, NLTK, Spacy,Spring Boot, Java. Samsung Research Institute Data Intelligence Trainee Bangalore, India | Jan 2020 - Jul 2020\nDeveloped an end-to-end system for processing Big Data and created a mobile application recommender system with Deep Autoencoders. Deployed the model with Django and created a python package for the tool. The model achieved P@K score of 0.83 Designed a Device Prediction project with user data to predict the device with usage statistics hitting 84% accuracy. Implemented CatBoost, LightGBM, XGBoost, and other different Boosting methods. Tech stack PySpark,Keras, Pytorch, Django, Tensorflow. Jones Lang LaSalle(JLL) Data Engineer(LaSalle Investment Management) Bangalore, India | Aug 2020 ‚Äì June 2021\nConsolidated data into Snowflake warehouse with snowpipes and used Snowflake sql for processing for a $5M ETL digitisation Integrated Looker with lookML and sql as well as PowerBI to build real time reporting metrics for different investment vehicles RESEARCH EXPERIENCE Graduate Research Assistant MedAn: A Framework for Investigating Live Medical Data against Privacy Laws Stony Brook, New York | Dec 2023 - Present\nMedAn: A Framework for Investigating Live Medical Data against Privacy Laws\nBuilding methods for deriving privacy properties from technical specs using semantic NLP, LLM/SLM Fine tuning, CoT and MoEs Medical Visual Question Answering with Vision Language Models\nBuilding a vision language model for Oncology Visual Question answering along with building an in house oncology VQA dataset. Developed routines to distributedly train large VLMs on a mixed in house HPC cluster consisting of NVIDIA H100s and DGX A100s using flash attention on Deepspeed. The model performs with 86% accuracy on closed and 77% recall on open VQA. Created a novel evaluation framework for the NLG evaluation of VLM by fine tuning LLM representations, achieving 0.82 correlation with existent scores on public med VQA datasets and a 68% coverage on all failure cases of existent metrics. Education Stony Brook University Stony Brook, U.S.A | Aug 2023- Present\nMaster of Science (M.S.) in Data Science | Statistical Learning \u0026amp; Computing | Big Data Systems | CGPA - 3.9/4 Manipal Institute of Technology Manipal, India | Aug 2016-Nov 2020\nBachelor Of Technology(B.Tech) in Computer Science and Engineering |Intelligent Systems | CGPA- 8.1/10 Projects SmartHome Savant: Explainable Anomaly Detector, LLM chatbot on QLORA PEFT RAG and Visual Layout Modellor MedZoom: A GAN based Super resolution network for Medical Images using Residual Map Attention generator network ","link":"https://abhihash01.github.io/post/profile/","section":"post","tags":["profile","resume","abhilash","featured"],"title":"Profile"},{"body":"","link":"https://abhihash01.github.io/tags/resume/","section":"tags","tags":null,"title":"Resume"},{"body":"FNU ABHILASH Data Scientist and Machine Learning Engineer with 3+ years of corporate experience, currently pursuing a Master‚Äôs in Data Science, and specializing in Generative AI, advanced NLP, CV, and recommendation systems.\nabhihash01@gmail.com | |\nSkills Languages: Python, C/C++, Java ML/DL Tools: PyTorch, Keras, Tensorflow, Numpy, Scikit, OpenCV, Pandas, Seaborn, CNN, RNN, LSTM, XGBoost, CatBoost Concepts: Data modeling, Statistical modeling, Regression, Natural Language Processing, Image Processing, Reinforcement Learning Generative AI: Langchain, LLamaindex, Ollama, Deepspeed, Torchtune, GAN, VAE, Diffusion, PEFT, QLoRA, RAG, Mixture of Experts (MoE), CoT/ Prompt Tuning, SFT, DPO, PPO, LLM Fine Tuning, RAG, CUDA, High Performance Computing, vLLM,OpenAI/Gemini API Ops: Docker, Kafka, Kubernates, Spring Boot, Django, Flask, FastAPI, Selenium, JIRA, Postman, Matlab, R, SAS, A/B Testing Data: Spark, PySpark, Hadoop, SQL, MySQL, MongoDB, Snowflake, Airflow, MLFlow, AWS, GCP, Sagemaker, Looker, PowerBI Experience SAP Machine Learning Engineer Bangalore, India | Jun 2021 - Aug 2023\nDeveloped a NLP based Biz Text Intelligence Model with NLTK and spaCy achieving 72% case coverage. Added Sentence Transformers for Intent Detection, increasing coverage to 94%, covering both rule based and natural conversation cases. Built a multi email based automatic email responder with contrastive learning using a pooled Transformers + CNN architecture and extractive summarizers achieving 0.67 ROUGE-L score on generation, 0.24 MAE and 0.87 NDCG for recommendation. Authored a product wide feature engineering library for 4 history use cases and implemented the Lead and Opportunity scoring model. The model provides scores along with what feature influences, enabling flexible strategy and model explainability. Lead a team for the migration of 8 ML use cases from SAP MLF to SAP AI Core directly impacting a revenue increase of $3.4M. Communicated with consumers and stakeholders to understand business requirements and spearheaded product roadmaps. Tech: Transformers, Catboost, PyTorch, Keras, Docker, Kubernates, AWS, Cloud Foundry, NLTK, Spacy,Spring Boot, Java. Samsung Research Institute Data Intelligence Trainee Bangalore, India | Jan 2020 - Jul 2020\nDeveloped an end-to-end system for processing Big Data and created a mobile application recommender system with Deep Autoencoders. Deployed the model with Django and created a python package for the tool. The model achieved P@K score of 0.83 Designed a Device Prediction project with user data to predict the device with usage statistics hitting 84% accuracy. Implemented CatBoost, LightGBM, XGBoost, and other different Boosting methods. Tech stack PySpark,Keras, Pytorch, Django, Tensorflow. Jones Lang LaSalle(JLL) Data Engineer(LaSalle Investment Management) Bangalore, India | Aug 2020 ‚Äì June 2021\nConsolidated data into Snowflake warehouse with snowpipes and used Snowflake sql for processing for a $5M ETL digitisation Integrated Looker with lookML and sql as well as PowerBI to build real time reporting metrics for different investment vehicles RESEARCH EXPERIENCE Graduate Research Assistant MedAn: A Framework for Investigating Live Medical Data against Privacy Laws Stony Brook, New York | Dec 2023 - Present\nMedAn: A Framework for Investigating Live Medical Data against Privacy Laws\nBuilding methods for deriving privacy properties from technical specs using semantic NLP, LLM/SLM Fine tuning, CoT and MoEs Medical Visual Question Answering with Vision Language Models\nBuilding a vision language model for Oncology Visual Question answering along with building an in house oncology VQA dataset. Developed routines to distributedly train large VLMs on a mixed in house HPC cluster consisting of NVIDIA H100s and DGX A100s using flash attention on Deepspeed. The model performs with 86% accuracy on closed and 77% recall on open VQA. Created a novel evaluation framework for the NLG evaluation of VLM by fine tuning LLM representations, achieving 0.82 correlation with existent scores on public med VQA datasets and a 68% coverage on all failure cases of existent metrics. Education Stony Brook University Stony Brook, U.S.A | Aug 2023- Present\nMaster of Science (M.S.) in Data Science | Statistical Learning \u0026amp; Computing | Big Data Systems | CGPA - 3.9/4 Manipal Institute of Technology Manipal, India | Aug 2016-Nov 2020\nBachelor Of Technology(B.Tech) in Computer Science and Engineering |Intelligent Systems | CGPA- 8.1/10 Projects SmartHome Savant: Explainable Anomaly Detector, LLM chatbot on QLORA PEFT RAG and Visual Layout Modellor MedZoom: A GAN based Super resolution network for Medical Images using Residual Map Attention generator network ","link":"https://abhihash01.github.io/post/resume/","section":"post","tags":["profile","resume","abhilash"],"title":"Resume"},{"body":"Desculpe, ainda estou aprendendo. Por favor, volte.\n","link":"https://abhihash01.github.io/about.pt/","section":"","tags":null,"title":"Sobre"}]