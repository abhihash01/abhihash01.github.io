[{"body":"","link":"https://abhihash01.github.io/","section":"","tags":null,"title":""},{"body":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Core Innovation Deep bidirectional context through joint conditioning on left/right context in all layers, overcoming limitations of:\nELMo's shallow concatenation GPT's unidirectional approach Key Architecture Components Model Specifications Parameter BERT-Base BERT-Large Layers 12 24 Hidden Dimension 768 1024 Attention Heads 12 16 Total Parameters 110M 340M Input Representation Three embedding types summation\nToken Embeddings: WordPiece (30k vocabulary) Segment Embeddings: Sentence A/B differentiation Position Embeddings: Learned positional encoding Special tokens:\n[CLS]: Classification token [SEP]: Sentence separator Pre Training- Fine Tuning Modelling\nPre-training Tasks 1. Masked Language Modeling (MLM) 15% tokens masked per sequence Masking Strategy: 80% ‚Üí [MASK] 10% ‚Üí Random token 10% ‚Üí Original token Solves pretraining-finetuning mismatch 2. Next Sentence Prediction (NSP) Binary classification task: 50% actual consecutive sentences 50% random sentence pairs Trains relationship understanding between text spans Training Details Data Sources BooksCorpus (800M words) English Wikipedia (2,500M words) Technical Specifications Training Hardware: 16 TPU pods Batch Size: 256 sequences √ó 512 tokens Adam Optimization (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999) Fine-tuning Approach Task-Specific Adaptations:\nSingle Sentence Tasks\nUse [CLS] token output for classification\nSentence Pair Tasks\nSeparate with [SEP] + segment embeddings\nToken-Level Tasks\nUse final hidden states directly\nImpact \u0026amp; Results Benchmark BERT-Large Previous SOTA Improvement GLUE 80.4% 72.7% +7.7% SQuAD v1.1 (F1) 93.2 91.6 +1.6 SWAG 86.3% 59.9% +26.4% Legacy \u0026amp; Evolution BERT's Influence:\nRoBERTa (Optimized pretraining) ALBERT (Parameter efficiency) DistilBERT (Knowledge distillation) Ethical Considerations Training Data: BookCorpus gender/cultural biases Carbon Footprint: ~1,500 kg CO‚ÇÇ equivalent Environmental Impact: 79 kWh per training hour Further Reading Original Paper: arXiv:1810.04805 Official GitHub: google-research/bert HuggingFace Implementation: transformers.BertModel ","link":"https://abhihash01.github.io/post/genai/primers/llm/bert/","section":"post","tags":["LLM","BERT"],"title":"Bert"},{"body":"","link":"https://abhihash01.github.io/tags/bert/","section":"tags","tags":null,"title":"BERT"},{"body":"","link":"https://abhihash01.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://abhihash01.github.io/tags/index/","section":"tags","tags":null,"title":"Index"},{"body":"","link":"https://abhihash01.github.io/tags/llm/","section":"tags","tags":null,"title":"LLM"},{"body":"","link":"https://abhihash01.github.io/series/llm/","section":"series","tags":null,"title":"LLM"},{"body":"","link":"https://abhihash01.github.io/categories/llm-series/","section":"categories","tags":null,"title":"LLM Series"},{"body":"","link":"https://abhihash01.github.io/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://abhihash01.github.io/categories/pretraining/","section":"categories","tags":null,"title":"Pretraining"},{"body":"","link":"https://abhihash01.github.io/categories/primer/","section":"categories","tags":null,"title":"Primer"},{"body":"","link":"https://abhihash01.github.io/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"https://abhihash01.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"Hi, I am a Data Scientist and Machine Learning Engineer with 3+ years of corporate experience, currently pursuing a Master‚Äôs in Data Science, and specializing in Generative AI, advanced NLP, CV, and recommendation systems.\nOne day while on my Research Assistant job, cramming through a set of research papers trying to understand a specific concept and come up with new ideas, I realised, I have a trail of knowledge that I am gaining through these search. But there is not place I keep it in an organised form. I learn a concept, apply, move on and then days later, I forget and try to scramble to get to the same resources that helped me build my concepts.\nHence I decided to make a central note repository of all the new concepts I learn. Whats a better way of accountability than to publish it at a central place. Hence the birth of this website.\nGoing forward, trying to take time out of my hectic schedule, I aim to continuously update stuff, mostly on Data Science, in an incremental curriculum manner, so anyone wanting to learn a concept like i did, can benefit the best.\nReach out to me for collaborations : abhihash01@gmail.com\n","link":"https://abhihash01.github.io/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://abhihash01.github.io/tags/hugo/","section":"tags","tags":null,"title":"Hugo"},{"body":"","link":"https://abhihash01.github.io/categories/syntax/","section":"categories","tags":null,"title":"Syntax"},{"body":"","link":"https://abhihash01.github.io/series/themes-guide/","section":"series","tags":null,"title":"Themes Guide"},{"body":"Page bundles are an optional way to organize page resources within Hugo.\nYou can opt-in to using page bundles in Hugo Clarity with usePageBundles in your site configuration or in a page's front matter. Read more about usePageBundles.\nWith page bundles, resources for a page or section, like images or attached files, live in the same directory as the content itself rather than in your static directory.\nHugo Clarity supports the use of leaf bundles, which are any directories within the content directory that contain an index.md file. Hugo's documentation gives this example:\n1content 2‚îú‚îÄ‚îÄ about 3‚îÇ ‚îú‚îÄ‚îÄ index.md 4‚îú‚îÄ‚îÄ posts 5‚îÇ ‚îú‚îÄ‚îÄ my-post 6‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ content1.md 7‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ content2.md 8‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ image1.jpg 9‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ image2.png 10‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.md 11‚îÇ ‚îî‚îÄ‚îÄ my-other-post 12‚îÇ ‚îî‚îÄ‚îÄ index.md 13‚îÇ 14‚îî‚îÄ‚îÄ another-section 15 ‚îú‚îÄ‚îÄ .. 16 ‚îî‚îÄ‚îÄ not-a-leaf-bundle 17 ‚îú‚îÄ‚îÄ .. 18 ‚îî‚îÄ‚îÄ another-leaf-bundle 19 ‚îî‚îÄ‚îÄ index.md In the above example `content` directory, there are four leaf bundles: about: This leaf bundle is at the root level (directly under content directory) and has only the index.md.\nmy-post: This leaf bundle has the index.md, two other content Markdown files and two image files. image1 is a page resource of my-post and only available in my-post/index.md resources. image2 is a page resource of my-post and only available in my-post/index.md resources.\nmy-other-post: This leaf bundle has only the index.md.\nanother-leaf-bundle: This leaf bundle is nested under couple of directories. This bundle also has only the index.md.\nThe hierarchy depth at which a leaf bundle is created does not matter, as long as it is not inside another leaf bundle.\nAdvantages to using page bundles The image below is part of the bundle of this page, and is located at content/post/bundle/building.png. Because it's within this page's bundle, the markup for the image only has to specify the image's filename, building.png.\nIf you ever change the name of the directory in which this Markdown file and the image reside, the reference to the image would not need to be updated.\nIn addition to more cleanly organizing your content and related assets, when using page bundles, Hugo Clarity will automatically generate markup for modern image formats, which are smaller in file size.\nFor instance, when you reference an image like building.png, Hugo Clarity will check to see if the same image (based on filename) exists in WebP, AVIF or JXL formats. If you inspect the image above, you'll see a \u0026lt;source\u0026gt; element for building.webp, because that file is also present. Hugo Clarity will only include the markup if these images exist.\nBrowsers that support these formats and the \u0026lt;picture\u0026gt; element will load them, while browsers that do not will fall-back to the default image. Read more about this process.\nFinally, note that page assets can be further managed and refined within the page's front matter if you wish, and are not limited to images alone.\nDisadvantages to using page bundles Page resources in a bundle are only available to the page with which they are bundled ‚Äî that means you can't include an image with one page and then reference it from another.\nImages that are being used in multiple places are more appropriate for your Hugo assets directory. Unlike files in the Hugo static directory, files in the assets directory can be run through Hugo Pipes, which includes image processing.\n","link":"https://abhihash01.github.io/post/bundle/","section":"post","tags":["Hugo"],"title":"Using Hugo page bundles"},{"body":"","link":"https://abhihash01.github.io/archives/","section":"","tags":null,"title":""},{"body":"","link":"https://abhihash01.github.io/tags/abhilash/","section":"tags","tags":null,"title":"Abhilash"},{"body":"","link":"https://abhihash01.github.io/tags/featured/","section":"tags","tags":null,"title":"Featured"},{"body":"","link":"https://abhihash01.github.io/tags/profile/","section":"tags","tags":null,"title":"Profile"},{"body":"","link":"https://abhihash01.github.io/categories/profile/","section":"categories","tags":null,"title":"Profile"},{"body":"FNU ABHILASH Data Scientist and Machine Learning Engineer with 3+ years of corporate experience, currently pursuing a Master‚Äôs in Data Science, and specializing in Generative AI, advanced NLP, CV, and recommendation systems.\nüìß abhihash01@gmail.com | üíº (https://www.linkedin.com/in/abhilash29998/) | üêô (https://github.com/abhihash01) | üåê https://abhihash01.github.io\nSkills Languages: Python, C/C++, Java ML/DL Tools: PyTorch, Keras, Tensorflow, Numpy, Scikit, OpenCV, Pandas, Seaborn, CNN, RNN, LSTM, XGBoost, CatBoost Concepts: Data modeling, Statistical modeling, Regression, Natural Language Processing, Image Processing, Reinforcement Learning Generative AI: Langchain, LLamaindex, Ollama, Deepspeed, Torchtune, GAN, VAE, Diffusion, PEFT, QLoRA, RAG, Mixture of Experts (MoE), CoT/ Prompt Tuning, SFT, DPO, PPO, LLM Fine Tuning, RAG, CUDA, High Performance Computing, vLLM,OpenAI/Gemini API Ops: Docker, Kafka, Kubernates, Spring Boot, Django, Flask, FastAPI, Selenium, JIRA, Postman, Matlab, R, SAS, A/B Testing Data: Spark, PySpark, Hadoop, SQL, MySQL, MongoDB, Snowflake, Airflow, MLFlow, AWS, GCP, Sagemaker, Looker, PowerBI Experience SAP Machine Learning Engineer Bangalore, India | Jun 2021 - Aug 2023\nDeveloped a NLP based Biz Text Intelligence Model with NLTK and spaCy achieving 72% case coverage. Added Sentence Transformers for Intent Detection, increasing coverage to 94%, covering both rule based and natural conversation cases. Built a multi email based automatic email responder with contrastive learning using a pooled Transformers + CNN architecture and extractive summarizers achieving 0.67 ROUGE-L score on generation, 0.24 MAE and 0.87 NDCG for recommendation. Authored a product wide feature engineering library for 4 history use cases and implemented the Lead and Opportunity scoring model. The model provides scores along with what feature influences, enabling flexible strategy and model explainability. Lead a team for the migration of 8 ML use cases from SAP MLF to SAP AI Core directly impacting a revenue increase of $3.4M. Communicated with consumers and stakeholders to understand business requirements and spearheaded product roadmaps. Tech: Transformers, Catboost, PyTorch, Keras, Docker, Kubernates, AWS, Cloud Foundry, NLTK, Spacy,Spring Boot, Java. Samsung Research Institute Data Intelligence Trainee Bangalore, India | Jan 2020 - Jul 2020\nDeveloped an end-to-end system for processing Big Data and created a mobile application recommender system with Deep Autoencoders. Deployed the model with Django and created a python package for the tool. The model achieved P@K score of 0.83 Designed a Device Prediction project with user data to predict the device with usage statistics hitting 84% accuracy. Implemented CatBoost, LightGBM, XGBoost, and other different Boosting methods. Tech stack PySpark,Keras, Pytorch, Django, Tensorflow. Jones Lang LaSalle(JLL) Data Engineer(LaSalle Investment Management) Bangalore, India | Aug 2020 ‚Äì June 2021\nConsolidated data into Snowflake warehouse with snowpipes and used Snowflake sql for processing for a $5M ETL digitisation Integrated Looker with lookML and sql as well as PowerBI to build real time reporting metrics for different investment vehicles RESEARCH EXPERIENCE Graduate Research Assistant MedAn: A Framework for Investigating Live Medical Data against Privacy Laws Stony Brook, New York | Dec 2023 - Present\nMedAn: A Framework for Investigating Live Medical Data against Privacy Laws\nBuilding methods for deriving privacy properties from technical specs using semantic NLP, LLM/SLM Fine tuning, CoT and MoEs Medical Visual Question Answering with Vision Language Models\nBuilding a vision language model for Oncology Visual Question answering along with building an in house oncology VQA dataset. Developed routines to distributedly train large VLMs on a mixed in house HPC cluster consisting of NVIDIA H100s and DGX A100s using flash attention on Deepspeed. The model performs with 86% accuracy on closed and 77% recall on open VQA. Created a novel evaluation framework for the NLG evaluation of VLM by fine tuning LLM representations, achieving 0.82 correlation with existent scores on public med VQA datasets and a 68% coverage on all failure cases of existent metrics. Education Stony Brook University Stony Brook, U.S.A | Aug 2023- Present\nMaster of Science (M.S.) in Data Science | Statistical Learning \u0026amp; Computing | Big Data Systems | CGPA - 3.9/4 Manipal Institute of Technology Manipal, India | Aug 2016-Nov 2020\nBachelor Of Technology(B.Tech) in Computer Science and Engineering |Intelligent Systems | CGPA- 8.1/10 Projects SmartHome Savant: Explainable Anomaly Detector, LLM chatbot on QLORA PEFT RAG and Visual Layout Modellor MedZoom: A GAN based Super resolution network for Medical Images using Residual Map Attention generator network ","link":"https://abhihash01.github.io/post/profile/","section":"post","tags":["profile","resume","abhilash","featured"],"title":"Profile"},{"body":"","link":"https://abhihash01.github.io/tags/resume/","section":"tags","tags":null,"title":"Resume"},{"body":"FNU ABHILASH Data Scientist and Machine Learning Engineer with 3+ years of corporate experience, currently pursuing a Master‚Äôs in Data Science, and specializing in Generative AI, advanced NLP, CV, and recommendation systems.\nabhihash01@gmail.com | |\nSkills Languages: Python, C/C++, Java ML/DL Tools: PyTorch, Keras, Tensorflow, Numpy, Scikit, OpenCV, Pandas, Seaborn, CNN, RNN, LSTM, XGBoost, CatBoost Concepts: Data modeling, Statistical modeling, Regression, Natural Language Processing, Image Processing, Reinforcement Learning Generative AI: Langchain, LLamaindex, Ollama, Deepspeed, Torchtune, GAN, VAE, Diffusion, PEFT, QLoRA, RAG, Mixture of Experts (MoE), CoT/ Prompt Tuning, SFT, DPO, PPO, LLM Fine Tuning, RAG, CUDA, High Performance Computing, vLLM,OpenAI/Gemini API Ops: Docker, Kafka, Kubernates, Spring Boot, Django, Flask, FastAPI, Selenium, JIRA, Postman, Matlab, R, SAS, A/B Testing Data: Spark, PySpark, Hadoop, SQL, MySQL, MongoDB, Snowflake, Airflow, MLFlow, AWS, GCP, Sagemaker, Looker, PowerBI Experience SAP Machine Learning Engineer Bangalore, India | Jun 2021 - Aug 2023\nDeveloped a NLP based Biz Text Intelligence Model with NLTK and spaCy achieving 72% case coverage. Added Sentence Transformers for Intent Detection, increasing coverage to 94%, covering both rule based and natural conversation cases. Built a multi email based automatic email responder with contrastive learning using a pooled Transformers + CNN architecture and extractive summarizers achieving 0.67 ROUGE-L score on generation, 0.24 MAE and 0.87 NDCG for recommendation. Authored a product wide feature engineering library for 4 history use cases and implemented the Lead and Opportunity scoring model. The model provides scores along with what feature influences, enabling flexible strategy and model explainability. Lead a team for the migration of 8 ML use cases from SAP MLF to SAP AI Core directly impacting a revenue increase of $3.4M. Communicated with consumers and stakeholders to understand business requirements and spearheaded product roadmaps. Tech: Transformers, Catboost, PyTorch, Keras, Docker, Kubernates, AWS, Cloud Foundry, NLTK, Spacy,Spring Boot, Java. Samsung Research Institute Data Intelligence Trainee Bangalore, India | Jan 2020 - Jul 2020\nDeveloped an end-to-end system for processing Big Data and created a mobile application recommender system with Deep Autoencoders. Deployed the model with Django and created a python package for the tool. The model achieved P@K score of 0.83 Designed a Device Prediction project with user data to predict the device with usage statistics hitting 84% accuracy. Implemented CatBoost, LightGBM, XGBoost, and other different Boosting methods. Tech stack PySpark,Keras, Pytorch, Django, Tensorflow. Jones Lang LaSalle(JLL) Data Engineer(LaSalle Investment Management) Bangalore, India | Aug 2020 ‚Äì June 2021\nConsolidated data into Snowflake warehouse with snowpipes and used Snowflake sql for processing for a $5M ETL digitisation Integrated Looker with lookML and sql as well as PowerBI to build real time reporting metrics for different investment vehicles RESEARCH EXPERIENCE Graduate Research Assistant MedAn: A Framework for Investigating Live Medical Data against Privacy Laws Stony Brook, New York | Dec 2023 - Present\nMedAn: A Framework for Investigating Live Medical Data against Privacy Laws\nBuilding methods for deriving privacy properties from technical specs using semantic NLP, LLM/SLM Fine tuning, CoT and MoEs Medical Visual Question Answering with Vision Language Models\nBuilding a vision language model for Oncology Visual Question answering along with building an in house oncology VQA dataset. Developed routines to distributedly train large VLMs on a mixed in house HPC cluster consisting of NVIDIA H100s and DGX A100s using flash attention on Deepspeed. The model performs with 86% accuracy on closed and 77% recall on open VQA. Created a novel evaluation framework for the NLG evaluation of VLM by fine tuning LLM representations, achieving 0.82 correlation with existent scores on public med VQA datasets and a 68% coverage on all failure cases of existent metrics. Education Stony Brook University Stony Brook, U.S.A | Aug 2023- Present\nMaster of Science (M.S.) in Data Science | Statistical Learning \u0026amp; Computing | Big Data Systems | CGPA - 3.9/4 Manipal Institute of Technology Manipal, India | Aug 2016-Nov 2020\nBachelor Of Technology(B.Tech) in Computer Science and Engineering |Intelligent Systems | CGPA- 8.1/10 Projects SmartHome Savant: Explainable Anomaly Detector, LLM chatbot on QLORA PEFT RAG and Visual Layout Modellor MedZoom: A GAN based Super resolution network for Medical Images using Residual Map Attention generator network ","link":"https://abhihash01.github.io/post/resume/","section":"post","tags":["profile","resume","abhilash"],"title":"Resume"},{"body":"Desculpe, ainda estou aprendendo. Por favor, volte.\n","link":"https://abhihash01.github.io/about.pt/","section":"","tags":null,"title":"Sobre"}]